# stockbot — technical README

This repository contains a Python-based experimental trading / research toolkit focused on LSTM-based predictive models for tick/market data and an opinionated live/demo runner. This README is written for technical users who want to understand internals, reproduce results, and safely customize the project.

## Table of contents

- Project overview
- Design contract (inputs, outputs, error modes)
- Repository layout (file-by-file technical summary)
- Data pipeline (raw -> features -> model-ready)
- Model architecture & training (high-level + where to customize)
- Running the project (env, commands, PowerShell tips)
- Customization checklist (hyperparameters, data, model, storage)
- Quality gates & testing recommendations
- Deployment, performance & GPU tips
- Troubleshooting
- Next steps and suggested improvements

---

## Project overview

This project experiments with time-series forecasting (LSTM) for short-term market signals. It contains data, training scripts, exported models, a live/demo runner, a small database for market snapshots, and utilities for exporting results to CSV. The code favors clarity over production robustness and is intended for research, backtesting experiments, and prototyping live visualizations.

Key features:

- Data ingestion and simple preprocessing for tick-level data (`tick_data.csv`, `market_data.db`).
- Training LSTM models and saving weights in Keras-compatible formats (`.h5`, `.keras`).
- A scaler artifact (`scaler.pkl`) used at training and inference time.
- Utilities for automated retraining and live plotting.

Note: The repository currently contains trained model files (`best_model.h5`, `final_model.h5`, etc). Treat them as artifacts — they are outputs, not source.

## Contract (short)

- Inputs: tabular tick or candle CSV (`tick_data.csv`) or a SQLite `market_data.db` containing time-series rows. Expect at least columns: timestamp, price (or open/high/low/close), volume. Exact schema depends on preprocessing code inside `train_lstm_model.py` and `auto_trainer.py`.
- Outputs: Keras model files (`*.h5`, `*.keras`), printed/logged metrics in `training_log.txt`, signals written to `signals.txt`, CSV exports from `export_to_csv.py`.
- Data shapes: training uses sliding windows (lookback). Typical shape: (batch_size, lookback, feature_count). The final dense head produces either a scalar prediction (next price) or classification (direction).
- Error modes: missing columns in data, mismatched scaler features, incompatible Keras/TensorFlow versions, GPU memory exhaustion during training.

## Repository layout (technical)

Top-level files (purpose):

- `Main.py` — primary script to run the live/opinionated workflow (integration point). Reads current model and scaler, produces signals, may write to `signals.txt`.
- `MainDemo.py` — a demo runner that can load demo inputs / run a local non-live flow for visualization or regression testing.
- `train_lstm_model.py` — training script implementing the data preprocessing pipeline, model definition (LSTM), training loop and checkpoints. Primary customization point for model hyperparameters and architecture.
- `auto_trainer.py` — orchestration script for scheduled or conditional retraining, model selection, and artifact rotation (e.g., promote best -> final).
- `export_to_csv.py` — utility to read DB or raw ticks and export analysis/predictions to CSV for external backtest or inspection.
- `live_chart.py` — visualization script using the selected model predictions for live plotting (matplotlib/pyplot likely).
- `scaler.pkl` — saved preprocessor (scikit-learn StandardScaler or similar). Must be consistent between training and inference.
- `market_data.db` — SQLite file that can store processed tick/candle rows.
- `tick_data.csv` — example/raw dataset used for training and experimentation. Acts as a canonical input for preprocessing.
- `signals.txt` — textual output of signals generated by `Main.py` or other run-time components.
- `*.h5` and `*.keras` files (`best_model.h5`, `final_model.h5`, `lstm_model.h5`, `best_model.keras`, `final_model.keras`) — saved models; keep multiple versions when experimenting.
- `requirements.txt` — pinned Python dependencies required to run the project (check this file before installing).
- `training_log.txt` — plain text log of training runs (loss, metrics, timestamps).
- `charts/training_curve.png` — example artifact showing training behaviour.

If you add new modules, keep them small and unit-testable. Prefer functions that accept and return pure Python/numpy objects rather than using global state.

## Data pipeline (technical details)

Typical pipeline implemented in `train_lstm_model.py` (canonical steps):

1. Data ingestion
   - Read either `tick_data.csv` or pull from `market_data.db`.
   - Ensure timestamp column parsed to timezone-aware `datetime` if needed.
2. Feature engineering
   - Compute OHLC windows, returns, log-returns, moving averages, RSI, or user-specified indicators.
   - Handle missing values via forward-fill or row dropping (configurable).
3. Scaling
   - Fit a `scikit-learn` scaler (likely `StandardScaler` or `MinMaxScaler`) on training features and serialize to `scaler.pkl`.
   - During inference, load `scaler.pkl` and apply transform to new data.
4. Sliding-window sequence creation
   - Create input sequences of length `LOOKBACK` producing targets at horizon `HORIZON`.
   - Typical shapes: X -> (N_windows, LOOKBACK, n_features); y -> (N_windows, target_dim).
5. Splitting
   - Train/validation split is temporal (do NOT random-shuffle across time). Reserve contiguous validation slice to avoid leakage.
6. Training
   - Build an LSTM (or stacked LSTMs), optional dropout, and a dense output.
   - Use appropriate loss (MSE, MAE, or classification cross-entropy) and metrics.
7. Checkpointing & selection
   - Save the best model (validation-based) to `best_model.h5` and optionally promote to `final_model.h5`.

Implementation notes:

- Temporal split: prefer `train_index = data.index < split_time` rather than random train_test_split.
- Keep scaler fit only on training set to avoid leakage.
- Persist preprocessing metadata (feature order) to ensure runtime consistency.

## Model architecture & where to customize

Default architecture (what to expect): stacked LSTM layers followed by a Dense head. Example hyperparameters to look for in `train_lstm_model.py`:

- LOOKBACK (sequence length): number of timesteps used as input.
- HORIZON (prediction horizon): number of steps ahead to predict.
- BATCH_SIZE, EPOCHS, LEARNING_RATE.
- LSTM units per layer: e.g., [64, 32].
- DROPOUT rate.
- LOSS function: 'mse' for regression, or a custom loss for directional accuracy.

To customize:

1. Open `train_lstm_model.py` and find the config block at the top or a `parse_args()` section. If there isn't one, add a small `argparse` or config dict to parameterize runs.
2. For architecture changes, look for code that builds the Keras model (calls to `tf.keras.layers.LSTM`, `Dense`, `Dropout`). Modify layer sizes or add BatchNormalization / Attention layers.
3. To change objective (classification vs regression), swap the final activation and loss. For example, binary classification -> `sigmoid` + `binary_crossentropy`.
4. When changing features, update `feature_columns` and refit a new `scaler.pkl` by re-running training.

Important: Whenever you update feature columns or their order, delete / re-create `scaler.pkl` and retrain models — the scaler encodes feature order and statistics.

## How to run (Windows PowerShell)

Follow these steps for a reproducible environment. All commands below assume PowerShell (v5.1) on Windows.

1) Create & activate a virtual environment

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

If PowerShell blocks scripts, enable the current session: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process`.

2) Install dependencies

```powershell
pip install --upgrade pip
pip install -r requirements.txt
```

3) Quick sanity checks

```powershell
python -c "import tensorflow as tf, sklearn; print(tf.__version__)"
python -c "import pickle, pandas as pd; print('ok')"
```

4) Train a model (edit hyperparams in `train_lstm_model.py` or via CLI if provided)

```powershell
python train_lstm_model.py
```

Expected outputs: updated `training_log.txt`, `scaler.pkl`, and a model file `best_model.h5` or `final_model.h5`.

5) Run the demo or live runner

```powershell
python MainDemo.py    # demo mode
python Main.py        # live/opinionated runner (ensure models/scaler are present)
python live_chart.py  # visualization only
```

6) Export data

```powershell
python export_to_csv.py
```

## Customization checklist (practical)

Where to change things and a safe workflow:

- To change TRAIN/VAL split: edit the time boundary in `train_lstm_model.py`. Use a date/time string or ratio and confirm with `pandas` slicing.
- To change LOOKBACK / HORIZON: update variables near the top of `train_lstm_model.py`. After changes, remove old model artifacts.
- To change features: update the list/columns used for X in the preprocessing block. Refit scaler.
- To try another architecture: extract model-building code into a factory function like `build_model(config)` and wire in a small config YAML/JSON.
- To run scheduled retraining: edit `auto_trainer.py` (looks for new data files or periodic retrain triggers). Add logging and safe-atomic file writes (write to temp file then rename).

Pro tips:

- Keep an experiment folder (e.g., `experiments/<YYYY-MM-DD>-<notes>/`) with hyperparams, `scaler.pkl`, and model to reproduce runs.
- Use deterministic seeds for numpy/tf to make small experiments reproducible: set `random.seed()`, `np.random.seed()`, and `tf.random.set_seed()` at the start of training.

## Quality gates & tests

Minimal recommended checks before merging changes:

- Linting: add `flake8`/`ruff` and run on changed modules.
- Type checks: add `mypy` if you start introducing typed APIs.
- Unit tests: add tests for preprocessing (scaler fit/transform round trip), sequence creation, and a tiny end-to-end train smoke test that runs 1 epoch on a tiny dataset. Place them under `tests/` and use `pytest`.

Example smoke test (conceptual):

- Build a deterministic 100-row sine-wave dataset.
- Run the pipeline to make sequences of LOOKBACK=10.
- Train 1 epoch with small batch.
- Assert model saves and `scaler.pkl` exists.

CI suggestion (GitHub Actions):

- Matrix: Python 3.10/3.11.
- Steps: install deps, run lint, run the smoke test only (not full training).

## Performance, GPU & deployment tips

- TensorFlow: If you rely on GPU, install matching `tensorflow`/`tensorflow-gpu` versions. On Windows, prefer wheels from official TF releases and ensure CUDA/cuDNN versions match.
- Memory: use `tf.data.Dataset` with prefetch/batching when dataset grows.
- Multi-GPU: use `tf.distribute.MirroredStrategy()` if you need parallel training.
- Docker: for deterministic deployments, build a Docker image with pinned Python and TF CUDA drivers, then run training inside the container.

Example Dockerfile outline (conceptual; not included):

- FROM python:3.10-slim
- RUN pip install -r requirements.txt
- COPY . /app
- WORKDIR /app
- ENTRYPOINT ["python", "Main.py"]

## Troubleshooting

- Incompatible model files: if saved with one TF/Keras version and loaded with another, you may see deserialization errors. Re-train or convert the model in the current runtime.
- Scaler mismatch: if features changed, old `scaler.pkl` will transform incorrectly. Recreate scaler and retrain.
- OOM during training: reduce batch size or model size; enable swap or use GPU with larger memory.
- ExecutionPolicy errors when activating venv on PowerShell: run `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process` in the same PowerShell session.

## Security & safe practices

- Never commit secrets, API keys, or exchange credentials to the repo.
- When deploying live trading code, incorporate kill-switches and dry-run modes to avoid unintended orders.
- Keep models and predictions auditable: log model version hash and scaler metadata alongside generated signals.

## Suggested next steps & improvements

1. Add a small `config.yaml` and a model factory so experiments can be run from config only.
2. Create `tests/` with at least three unit tests: preprocessing, sequence generation, and a 1-epoch train smoke test.
3. Add a reproducible Docker setup and a GitHub Actions workflow for CI smoke tests.
4. Add an `experiments/` manifest that records hyperparams, git commit hash, and dataset snapshot for each run.

## Appendix: Quick reference commands (PowerShell)

Activate venv and install:

```powershell
python -m venv .venv; .\.venv\Scripts\Activate.ps1; pip install --upgrade pip; pip install -r requirements.txt
```

Train:

```powershell
python train_lstm_model.py
```

Run demo / main:

```powershell
python MainDemo.py
python Main.py
```

Export CSV:

```powershell
python export_to_csv.py
```

---

If you want, I can also:

- add a small `tests/` folder with a smoke test and CI workflow, or
- extract hyperparameters into a `config.yaml` and update training code to consume it, or
- create a Dockerfile + example GitHub Actions workflow.

Tell me which of the above you'd like me to add next and I'll implement it.
